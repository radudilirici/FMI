{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## In this lab we are going to implement the following algorithms from the course:\n",
    " - policy evaluation\n",
    " - policy improvement\n",
    " - policy iteration\n",
    " - value iteration\n",
    " \n",
    "You can review these either from the course, or read Chapter 4 from the book Introduction to Reinforcement Learning by Sutton (attached to Files section in Teams)\n",
    " \n",
    "We'll work on the Frozen Lake environment: https://gym.openai.com/envs/FrozenLake-v0/\n",
    "Read the description, but look at their github implementation of the environment later, after implementing this lab.\n",
    "For now, follow the code in the cell below, function `runEpisode' to see how we load and interact with this environment\n",
    "Remember that all environments from gym have a similar structure and it is important to understand the API !\n",
    "\n",
    "### Note that in order to solve our problems using Bellman equations (DP method) as we do in this laboratory means two things:\n",
    " - We have full access to the model dynamics - which we do, as you see below with matrix P. \n",
    "   In the continuation of the course, as in most of the problems this information is not available because it is very difficult to obtain !\n",
    " - We can represent numerically all the states and actions. Is this feasible for a self driving car ?\n",
    " - However, when the above requirements can be satisfied, NOTE that the solutions are OPTIMAL !. Choose your algorithms wisely !\n",
    " \n",
    "### Now, read the self-explanatory cells below and write code only in the places marked with \"YOUR IMPLEMENTATION HERE\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.18.1)\n",
      "Collecting gym\n",
      "  Downloading gym-0.17.3.tar.gz (1.6 MB)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.18.1)\n",
      "Collecting pyglet<=1.5.0,>=1.4.0\n",
      "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.3.0)\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py): started\n",
      "  Building wheel for gym (setup.py): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654657 sha256=d14398e56380497fd2a6eb6653af05e933011154a34f0d7a6a27e0a394c99623\n",
      "  Stored in directory: c:\\users\\mr. wombat\\appdata\\local\\pip\\cache\\wheels\\d1\\81\\4b\\dd9c029691022cb957398d1f015e66b75e37637dda61abdf58\n",
      "Successfully built gym\n",
      "Installing collected packages: pyglet, gym\n",
      "Successfully installed gym-0.17.3 pyglet-1.5.0\n"
     ]
    }
   ],
   "source": [
    "# First ensure that we can install numpy and gym here then import them\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install gym\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### This is how we load the environment. We give it the name and the parameters (which are sent to the __init__ function of the respective class)\n",
    "### Try to switch the name of the map between 4x4 and 8x8 to see performance.\n",
    "### is_splippery = False means the environment is deterministic, while True means that doing an action \"a\" in state \"s\" can cause movement to different states \"s'\" ! (non-deterministic !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\",  map_name=\"8x8\", is_slippery=False)\n",
    "\n",
    "\"\"\"\n",
    "You can see from documentation that this environment contains three main things inside:\n",
    "    P: nested dictionary \n",
    "        (simulates the  p(s',r | s, a) = the probability of being in state s, applying action a and landing in state s' with a reward of r)\n",
    "        From gym.core.Environment:\n",
    "        For each pair of states in [1, nS] and actions in [1, nA], P[state][action] is a\n",
    "        tuple of the form (probability, nextstate, reward, terminal) where\n",
    "            - probability: float\n",
    "                the probability of transitioning from \"state\" to \"nextstate\" with \"action\"\n",
    "            - nextstate: int\n",
    "                denotes the state we transition to (in range [0, nS - 1])\n",
    "            - reward: int\n",
    "                either 0 or 1, the reward for transitioning from \"state\" to\n",
    "                \"nextstate\" with \"action\"\n",
    "            - terminal: bool\n",
    "              True when \"nextstate\" is a terminal state (hole or goal), False otherwise\n",
    "    nS: int\n",
    "        number of states in the environment\n",
    "    nA: int\n",
    "        number of actions in the environment\n",
    "        Inside, they implement it with an enum:\n",
    "        LEFT = 0\n",
    "        DOWN = 1\n",
    "        RIGHT = 2\n",
    "        UP = 3\n",
    "\"\"\"\n",
    "\n",
    "def runEpisode(env, policy, maxSteps=100):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "    \"\"\"\n",
    "    \n",
    "    # We count here the total\n",
    "    total_reward = 0\n",
    "    \n",
    "    # THis is how we reset the environment to an initial state, it returns the observation.\n",
    "    # As documented, in this case the observation is the state where the agent currently is positionaed, \n",
    "    #, which is a number in [0, nS-1]. We can use local function stateToRC to get the row and column of the agent\n",
    "    # The action give is in range [0, nA-1], check the enum defined above to understand what each number means\n",
    "    obs = env.reset() \n",
    "    for t in range(maxSteps):\n",
    "        # Draw the environment on screen\n",
    "        env.render() \n",
    "        # Sleep a bit between decisions\n",
    "        time.sleep(0.25)\n",
    "        \n",
    "        # Here we sample an action from our policy, we consider it deterministically at this point\n",
    "        action = policy[obs]\n",
    "        \n",
    "        # Hwere we interact with the enviornment. We give it an action to do and it returns back:\n",
    "        # - the new observation (observable state by the agent),\n",
    "        # - the reward of the action just made\n",
    "        # - if the simulation is done (terminal state)\n",
    "        # - last parameters is an \"info\" output, we are not interested in this one that's why we ignore the parameter\n",
    "        newObs, reward, done, _ = env.step(action)\n",
    "        print(f\"Agent was in state {obs}, took action {action}, now in state {newObs}\")\n",
    "        obs = newObs\n",
    "        \n",
    "        total_reward += reward\n",
    "        # Close the loop before maxSteps  if we are in a terminal state\n",
    "        if done:\n",
    "            break\n",
    "   \n",
    "    if not done:   \n",
    "        print(f\"The agent didn't reach a terminal state in {maxSteps} steps.\")\n",
    "    else:\n",
    "        print(f\"Episode reward: {total_reward}\")\n",
    "    env.render() # One last  rendering of the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment the code below to  create a random policy and see an episode in action.\n",
    "# Run it several times and see your agent in action with a random deterministic policy :)\n",
    "# random_policy = np.random.choice(env.nA, size=(env.nS,))\n",
    "# print(random_policy)\n",
    "# runEpisode(env, random_policy, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md \n"
    }
   },
   "outputs": [],
   "source": [
    "### Now let's run the episode with this policy found by value iteration\n",
    "### Note that you may need to increase max number of Steps or run it several times if you select is_slippery !\n",
    "### Try to play with map namees and stochastic parameter is_slippery than run again all above cells including this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting policy improvement\n",
      "Policy evaluation converged after 1 iterations\n",
      "Policy evaluation converged after 2 iterations\n",
      "Policy evaluation converged after 3 iterations\n",
      "Policy evaluation converged after 4 iterations\n",
      "Policy evaluation converged after 5 iterations\n",
      "Policy evaluation converged after 6 iterations\n",
      "Policy evaluation converged after 7 iterations\n",
      "Policy evaluation converged after 8 iterations\n",
      "Policy evaluation converged after 9 iterations\n",
      "Policy evaluation converged after 10 iterations\n",
      "Policy evaluation converged after 11 iterations\n",
      "Policy evaluation converged after 12 iterations\n",
      "Policy evaluation converged after 13 iterations\n",
      "Policy evaluation converged after 14 iterations\n",
      "Policy evaluation converged after 15 iterations\n",
      "Policy is done improving\n"
     ]
    }
   ],
   "source": [
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"Evaluate the value function from a given policy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    policy: np.array[nS]\n",
    "        The policy to evaluate. Maps states to actions, deterministic !\n",
    "    tol: float\n",
    "        Terminate policy evaluation when\n",
    "            max |value_function(s) - prev_value_function(s)| < tol\n",
    "    Returns\n",
    "    -------\n",
    "    value_function: np.ndarray[nS]\n",
    "        The value function of the given policy, where value_function[s] is\n",
    "        the value of state s\n",
    "    \"\"\"\n",
    "    # Init with 0 for all states, \n",
    "    # Remember that terminal states MUST have 0 always whatever you initialize them with here\n",
    "    value_function = np.zeros(nS) \n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    numIters = 0\n",
    "    while True:\n",
    "        numIters += 1\n",
    "        max_delta = -1\n",
    "        for s in range(nS):\n",
    "            a = policy[s]\n",
    "            new_value_function = 0.0\n",
    "            for possible_move in P[s][a]:\n",
    "                probability, next_state, reward, _ = possible_move\n",
    "                new_value_function += probability * (reward + gamma * value_function[next_state])\n",
    "                \n",
    "            max_delta = max(max_delta, abs(new_value_function - value_function[s]))\n",
    "            value_function[s] = new_value_function\n",
    "        if max_delta < tol:\n",
    "            break\n",
    "            \n",
    "    print(f\"Policy evaluation converged after {numIters} iterations\")\n",
    "        \n",
    "    ############################\n",
    "\n",
    "    return value_function\n",
    "\n",
    "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
    "    \"\"\"Given the value function from policy improve the policy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    value_from_policy: np.ndarray\n",
    "        The value calculated from the policy\n",
    "    policy: np.array\n",
    "        The previous policy.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    new_policy: np.ndarray[nS]\n",
    "        An array of integers. Each integer is the optimal action to take\n",
    "        in that state according to the environment dynamics and the\n",
    "        given value function.\n",
    "    \"\"\"\n",
    "\n",
    "    new_policy = np.zeros(nS, dtype='int') # Default is left action\n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    for s in range(nS):\n",
    "        value_per_action = np.zeros(shape=(nA, 1))\n",
    "        for a in range(nA):\n",
    "            action_value = 0.0\n",
    "            for possible_move in P[s][a]:\n",
    "                probability, next_state, reward, _ = possible_move\n",
    "                action_value += probability * (reward + gamma * value_from_policy[next_state])\n",
    "            value_per_action[a] = action_value\n",
    "        best_action_idx = np.argmax(value_per_action)\n",
    "        new_policy[s] = best_action_idx\n",
    "\n",
    "    ############################\n",
    "    return new_policy\n",
    "\n",
    "\n",
    "def policy_iteration(P, nS, nA, gamma=0.9, tol=10e-3):\n",
    "    \"\"\"Runs policy iteration.\n",
    "        You should call the policy_evaluation() and policy_improvement() methods to\n",
    "        implement this method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        P, nS, nA, gamma:\n",
    "            defined at beginning of file\n",
    "        tol: float\n",
    "            tol parameter used in policy_evaluation()\n",
    "        Returns:\n",
    "        ----------\n",
    "        value_function: np.ndarray[nS]\n",
    "        policy: np.ndarray[nS]\n",
    "    \"\"\"\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    print(\"Starting policy improvement\")\n",
    "    while True:\n",
    "        value_function = policy_evaluation(P, nS, nA, policy, gamma, tol)\n",
    "        new_policy = policy_improvement(P, nS, nA, value_function, policy, gamma)\n",
    "        if not np.any(new_policy != policy) :\n",
    "            break\n",
    "        policy = new_policy\n",
    "    print(\"Policy is done improving\")\n",
    "    ############################\n",
    "    return value_function, policy\n",
    "\n",
    "\n",
    "gamma = 0.9\n",
    "best_V, best_PI = policy_iteration(env.P, env.nS, env.nA, gamma=gamma, tol=10e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Agent was in state 0, took action 1, now in state 8\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Agent was in state 8, took action 1, now in state 16\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Agent was in state 16, took action 1, now in state 24\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "\u001b[41mF\u001b[0mFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Agent was in state 24, took action 2, now in state 25\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "F\u001b[41mF\u001b[0mFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Agent was in state 25, took action 2, now in state 26\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FF\u001b[41mF\u001b[0mFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Agent was in state 26, took action 2, now in state 27\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFF\u001b[41mF\u001b[0mFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Agent was in state 27, took action 2, now in state 28\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFF\u001b[41mF\u001b[0mHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Agent was in state 28, took action 1, now in state 36\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFH\u001b[41mF\u001b[0mFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Agent was in state 36, took action 1, now in state 44\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHF\u001b[41mF\u001b[0mFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Agent was in state 44, took action 2, now in state 45\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFF\u001b[41mF\u001b[0mHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "Agent was in state 45, took action 1, now in state 53\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFH\u001b[41mF\u001b[0mHF\n",
      "FFFHFFFG\n",
      "Agent was in state 53, took action 1, now in state 61\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHF\u001b[41mF\u001b[0mFG\n",
      "Agent was in state 61, took action 2, now in state 62\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFF\u001b[41mF\u001b[0mG\n",
      "Agent was in state 62, took action 2, now in state 63\n",
      "Episode reward: 1.0\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Now let's run the episode with this policy found !\n",
    "runEpisode(env, policy=best_PI, maxSteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration converged after 7\n"
     ]
    }
   ],
   "source": [
    "# Now let's implement value iteration algorithm, which in general can converge faster !\n",
    "def value_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Learn value function and policy by using value iteration method for a given\n",
    "    gamma and environment.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    tol: float\n",
    "        Terminate value iteration when\n",
    "            max |value_function(s) - prev_value_function(s)| < tol\n",
    "    Returns:\n",
    "    ----------\n",
    "    value_function: np.ndarray[nS]\n",
    "    policy: np.ndarray[nS]\n",
    "    \"\"\"\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    \n",
    "    \n",
    "\n",
    "    ############################\n",
    "    return value_function, policy\n",
    "\n",
    "gamma = 0.9\n",
    "best_value, best_policy = value_iteration(env.P, env.nS, env.nA, gamma=gamma, tol=10e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Now let's run the episode with this policy found by value iteration\n",
    "### Note that you may need to increase max number of Steps or run it several times if you select is_slippery !\n",
    "### Try to play with map names and stochastic parameter is_slippery than run again all above cells including this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Agent was in state 0, took action 1, now in state 4\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Agent was in state 4, took action 1, now in state 8\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "Agent was in state 8, took action 2, now in state 9\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "Agent was in state 9, took action 1, now in state 13\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "Agent was in state 13, took action 2, now in state 14\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "Agent was in state 14, took action 2, now in state 15\n",
      "Episode reward: 1.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "runEpisode(env, policy=best_PI, maxSteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
