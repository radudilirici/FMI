{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import time\n",
    "\n",
    "# Imports specifically so we can render outputs in Jupyter.\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "# from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparams\n",
    "\n",
    "# In how many slices discretize the continuous space, the bigger, the smoother. but it increases a lot the time to converge !\n",
    "# Try and check to see how they work !\n",
    "numStates_pos = 10\n",
    "numStates_speed = 20\n",
    "numStates = np.array([numStates_pos, numStates_speed])\n",
    "\n",
    "# The environment low, high and interval mapped per state\n",
    "env_low = None\n",
    "env_high = None\n",
    "env_dx = None\n",
    "\n",
    "# Number of episodes\n",
    "numEpisodes = 5000\n",
    "maxStepsPerEpisode = 200 #Number of max actions taken per episode. If in 200 steps it's not done, the environment takes it as fail.\n",
    "\n",
    "# Tweaking params\n",
    "initial_lr = 1.0 # Initial Learning Rate\n",
    "min_lr = 0.001 # Minimum Learning Rate\n",
    "lr_decay = 0.9996\n",
    "gamma = 1.0 # Discount factor\n",
    "epsilon_start = 1.0 # Allow the model to do a lot of trial and error on the beggining\n",
    "epsilon_decay = 0.999 # Decay per episode.\n",
    "epsilon_end = 0.01 # The end point / min of the epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get epsilon by Episode\n",
    "def get_epsilon(n_episode):\n",
    "    epsilon = max(epsilon_start * (epsilon_decay ** n_episode), epsilon_end)\n",
    "    return (epsilon)\n",
    "\n",
    "\n",
    "def obs_to_state(obs):\n",
    "    \"\"\" Maps an observation to state \"\"\"\n",
    "\n",
    "    position = int((obs[0] - env_low[0])/env_dx[0])\n",
    "    speed = int((obs[1] - env_low[1])/env_dx[1])\n",
    "    return position, speed\n",
    "\n",
    "\n",
    "def run_episode(env, policy=None, render=False):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    frames = []\n",
    "    for _ in range(maxStepsPerEpisode):\n",
    "        if render:\n",
    "            frames.append(env.render(mode='rgb_array'))\n",
    "        if policy is None:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            pos, speed = obs_to_state(obs)\n",
    "            action = policy[pos][speed]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += gamma ** step_idx * reward\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if render:\n",
    "        env.render()\n",
    "        env.close()\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select action based on epsilon greedy\n",
    "def select_action(env, q_table, state, epsilon):\n",
    "    position, speed = state\n",
    "    # implicit policy; if we have action values for that state, choose the largest one, else random\n",
    "    if np.random.rand() > epsilon:\n",
    "#         logits = q_table[position][speed]\n",
    "#         logits_exp = np.exp(logits)\n",
    "#         probs = logits_exp / np.sum(logits_exp)\n",
    "#         action = np.random.choice(env.action_space.n, p=probs)  # asa era la prof\n",
    "        action = np.argmax(q_table[position][speed])\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "    return action\n",
    "\n",
    "# Given (state, action, reward, next_state) pair after a transition made in the e nvironment and the episode index\n",
    "def updateExperience(env, q_table, state, action, reward, next_state, lr):\n",
    "#     next_action = select_action(env, q_table, next_state, -1)  # -1 so the algorithm NEVER chooses random on second action\n",
    "    # Q(s t+1, a t+1)\n",
    "    position, speed = state\n",
    "    next_position, next_speed = next_state\n",
    "    next_q = np.max(q_table[next_position][next_speed])\n",
    "    q_table[position][speed][action] += lr * (reward + gamma * next_q - q_table[position][speed][action])\n",
    "\n",
    "def train_q_learning(env):\n",
    "    print('Start Q-Learning training:')\n",
    "    display_freq = min(numEpisodes // 10, 1000)\n",
    "\n",
    "    # Initialize Q-Table\n",
    "    q_table = np.random.uniform(-1, 1, (numStates[0], numStates[1], 3))  # [number_of_positions x number_of_speeds x number_of_actionst]\n",
    "    last100_moving_total = 0\n",
    "    last100_rewards = deque()\n",
    "    SOLVED = False\n",
    "    last_total_rewards = [] # For stat purposes, accumultates some episode rewards\n",
    "    \n",
    "    for episode in range(numEpisodes):\n",
    "        eps = get_epsilon(episode)\n",
    "        lr = max(min_lr, initial_lr * (lr_decay ** episode))\n",
    "        \n",
    "        obs = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(maxStepsPerEpisode):\n",
    "            state = obs_to_state(obs)\n",
    "            action = select_action(env, q_table, state, eps)\n",
    "            \n",
    "            # step environment\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            next_state = obs_to_state(obs)\n",
    "            total_reward += reward\n",
    "            \n",
    "            updateExperience(env, q_table, state, action, reward, next_state, lr)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        last100_rewards.append(total_reward)\n",
    "        last100_moving_total += total_reward\n",
    "        \n",
    "        while len(last100_rewards) > 100:\n",
    "            removedItem = last100_rewards.popleft()\n",
    "            last100_moving_total -= removedItem\n",
    "            \n",
    "        last100_moving_avg = last100_moving_total / len(last100_rewards)\n",
    "        if len(last100_rewards) >= 100 and last100_moving_total >= -120:\n",
    "            print(f\"We solved the game at episode {episode} !\")\n",
    "            SOLVED = True\n",
    "            break\n",
    "\n",
    "\n",
    "        if episode % display_freq == 0:  # Write out partial results\n",
    "            print(f'At episode: {episode+1} - Reward mean from last 100 episodes: {last100_moving_avg}. - LR:{lr:0.4f} - eps:{eps:0.4f}')\n",
    "            last_total_rewards.clear()\n",
    "        \n",
    "    print('Training finished!')\n",
    "    solution_policy = np.argmax(q_table, axis=2)\n",
    "    solution_policy_scores = [run_episode(env, solution_policy, False) for _ in range(1000)]\n",
    "    print(\"Average score of solution on a dry run= \", np.mean(solution_policy_scores))\n",
    "\n",
    "    return solution_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Q-Learning training:\n",
      "At episode: 1 - Reward mean from last 100 episodes: -200.0. - LR:1.0000 - eps:1.0000\n",
      "At episode: 501 - Reward mean from last 100 episodes: -200.0. - LR:0.8187 - eps:0.6064\n",
      "At episode: 1001 - Reward mean from last 100 episodes: -200.0. - LR:0.6703 - eps:0.3677\n",
      "At episode: 1501 - Reward mean from last 100 episodes: -200.0. - LR:0.5487 - eps:0.2230\n",
      "At episode: 2001 - Reward mean from last 100 episodes: -199.68. - LR:0.4493 - eps:0.1352\n",
      "At episode: 2501 - Reward mean from last 100 episodes: -197.68. - LR:0.3678 - eps:0.0820\n",
      "At episode: 3001 - Reward mean from last 100 episodes: -197.93. - LR:0.3011 - eps:0.0497\n",
      "At episode: 3501 - Reward mean from last 100 episodes: -198.52. - LR:0.2465 - eps:0.0301\n",
      "At episode: 4001 - Reward mean from last 100 episodes: -166.19. - LR:0.2018 - eps:0.0183\n",
      "At episode: 4501 - Reward mean from last 100 episodes: -139.98. - LR:0.1652 - eps:0.0111\n",
      "Training finished!\n",
      "Average score of solution on a dry run=  -180.745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-191.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# Read the env things\n",
    "env_low = env.observation_space.low\n",
    "env_high = env.observation_space.high\n",
    "env_dx = (env_high - env_low) / numStates\n",
    "\n",
    "# Train a policy. TODO: save it\n",
    "sol_policy = train_q_learning(env)\n",
    "\n",
    "\n",
    "# Play  simulation with the learned policy\n",
    "run_episode(env, sol_policy, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('policy_2.npy', 'wb') as f:\n",
    "    np.save(f, sol_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
