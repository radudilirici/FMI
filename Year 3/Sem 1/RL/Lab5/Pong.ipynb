{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import time\n",
    "import cv2\n",
    "#from skimage.color import rgb2gray\n",
    "\n",
    "# Imports specifically so we can render outputs in Jupyter.\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'pong_policy.npy'\n",
    "### Hyperparams\n",
    "\n",
    "# In how many slices discretize the continuous space, the bigger, the smoother. but it increases a lot the time to converge !\n",
    "# Try and check to see how they work !\n",
    "numStates_x = 60\n",
    "numStates_y = 80\n",
    "numStates = np.array([numStates_x, numStates_y, numStates_x, numStates_y, 3, 30])\n",
    "\n",
    "# The environment low, high and interval mapped per state\n",
    "env_low = None\n",
    "env_high = None\n",
    "env_dx = None\n",
    "\n",
    "# Number of episodes\n",
    "numEpisodes = 100\n",
    "\n",
    "# Tweaking params\n",
    "initial_lr = 1.0 # Initial Learning Rate\n",
    "min_lr = 0.001 # Minimum Learning Rate\n",
    "lr_decay = 0.999996\n",
    "gamma = 1.0 # Discount factor\n",
    "epsilon_start = 1.0 # Allow the model to do a lot of trial and error on the beggining\n",
    "epsilon_decay = 0.999 # Decay per episode.\n",
    "epsilon_end = 0.01 # The end point / min of the epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_to_state(obs):\n",
    "    \"\"\" Maps an observation to state \"\"\"\n",
    "\n",
    "    p_x = int(obs[0] / env_dx[0])\n",
    "    p_y = int(obs[1] / env_dx[1])\n",
    "    ball_x = int(obs[2] / env_dx[0])\n",
    "    ball_y = int(obs[3] / env_dx[1])\n",
    "    ball_dir_x = int(obs[4] + 1)\n",
    "    ball_dir_y = int((obs[5] + 1.5) * 10)\n",
    "    \n",
    "    return p_x, p_y, ball_x, ball_y, ball_dir_x, ball_dir_y\n",
    "\n",
    "# get epsilon by Episode\n",
    "def get_epsilon(n_episode):\n",
    "    epsilon = max(epsilon_start * (epsilon_decay ** n_episode), epsilon_end)\n",
    "    return (epsilon)\n",
    "\n",
    "\n",
    "def run_episode(env, policy=None, render=False):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    images = []\n",
    "    while True:\n",
    "        if policy is None:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            p_x, p_y, ball_x, ball_y, ball_dir_x, ball_dir_y = obs_to_state(obs)\n",
    "            action = policy[p_x][p_y][ball_x][ball_y][ball_dir_x][ball_dir_y]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += gamma ** step_idx * reward\n",
    "        step_idx += 1\n",
    "        if render:\n",
    "            images.append(env.render())\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward, np.array(images, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select action based on epsilon greedy\n",
    "def select_action(env, q_table, state, epsilon):\n",
    "    p_x, p_y, ball_x, ball_y, ball_dir_x, ball_dir_y = state\n",
    "    # implicit policy; if we have action values for that state, choose the largest one, else random\n",
    "    if np.random.rand() > epsilon:\n",
    "#         logits = q_table[position][speed]\n",
    "#         logits_exp = np.exp(logits)\n",
    "#         probs = logits_exp / np.sum(logits_exp)\n",
    "#         action = np.random.choice(env.action_space.n, p=probs)  # asa era la prof\n",
    "        action = np.argmax(q_table[p_x][p_y][ball_x][ball_y][ball_dir_x][ball_dir_y])\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "    return action\n",
    "\n",
    "# Given (state, action, reward, next_state) pair after a transition made in the e nvironment and the episode index\n",
    "def updateExperience(env, q_table, state, action, reward, next_state, lr):\n",
    "#     next_action = select_action(env, q_table, next_state, -1)  # -1 so the algorithm NEVER chooses random on second action\n",
    "    # Q(s t+1, a t+1)\n",
    "    p_x, p_y, ball_x, ball_y, ball_dir_x, ball_dir_y = state\n",
    "    p_x_n, p_y_n, ball_x_n, ball_y_n, ball_dir_x_n, ball_dir_y_n = next_state\n",
    "    next_q = np.max(q_table[p_x_n][p_y_n][ball_x_n][ball_y_n][ball_dir_x_n][ball_dir_y_n])\n",
    "    q_table[p_x][p_y][ball_x][ball_y][ball_dir_x][ball_dir_y][action] += lr * (reward + gamma * next_q - q_table[p_x][p_y][ball_x][ball_y][ball_dir_x][ball_dir_y][action])\n",
    "\n",
    "def train_q_learning(env):\n",
    "    print('Start Q-Learning training:')\n",
    "    display_freq = min(numEpisodes // 10, 1000)\n",
    "\n",
    "    # Initialize Q-Table\n",
    "    q_table = np.random.uniform(-1, 1, (numStates[0], numStates[1], numStates[2], numStates[3], numStates[4], numStates[5], 3))  # [number_of_positions x number_of_speeds x number_of_actions]\n",
    "    last100_moving_total = 0\n",
    "    last100_rewards = deque()\n",
    "    SOLVED = False\n",
    "    last_total_rewards = [] # For stat purposes, accumultates some episode rewards\n",
    "    \n",
    "    for episode in range(numEpisodes):\n",
    "        eps = get_epsilon(episode)\n",
    "        lr = max(min_lr, initial_lr * (lr_decay ** episode))\n",
    "        \n",
    "        obs = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            state = obs_to_state(obs)\n",
    "            action = select_action(env, q_table, state, eps)\n",
    "            \n",
    "            # step environment\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            next_state = obs_to_state(obs)\n",
    "            total_reward += reward\n",
    "            \n",
    "            updateExperience(env, q_table, state, action, reward, next_state, lr)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        last100_rewards.append(total_reward)\n",
    "        last100_moving_total += total_reward\n",
    "        \n",
    "        while len(last100_rewards) > 100:\n",
    "            removedItem = last100_rewards.popleft()\n",
    "            last100_moving_total -= removedItem\n",
    "                        \n",
    "        last100_moving_avg = last100_moving_total / len(last100_rewards)\n",
    "\n",
    "        if episode % display_freq == 0:  # Write out partial results\n",
    "            print(f'At episode: {episode+1} - Reward mean from last 100 episodes: {last100_moving_avg}. - LR:{lr:0.4f} - eps:{eps:0.4f}')\n",
    "            last_total_rewards.clear()\n",
    "        \n",
    "    print('Training finished!')\n",
    "    solution_policy = np.argmax(q_table, axis=4)\n",
    "    solution_policy_scores = [run_episode(env, solution_policy, False)[0] for _ in range(1000)]\n",
    "    print(\"Average score of solution on a dry run= \", np.mean(solution_policy_scores))\n",
    "\n",
    "    return solution_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Q-Learning training:\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 46.3 GiB for an array with shape (60, 80, 60, 80, 3, 30, 3) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-2060f4b46fc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Train a policy. TODO: save it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msol_policy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_q_learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-2eb6eb120ecf>\u001b[0m in \u001b[0;36mtrain_q_learning\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# Initialize Q-Table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mq_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnumStates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumStates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumStates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumStates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumStates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumStates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [number_of_positions x number_of_speeds x number_of_actions]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mlast100_moving_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mlast100_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.uniform\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_common.pyx\u001b[0m in \u001b[0;36mnumpy.random._common.cont\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 46.3 GiB for an array with shape (60, 80, 60, 80, 3, 30, 3) and data type float64"
     ]
    }
   ],
   "source": [
    "import pong_env_pos as pong_env\n",
    "env = pong_env.env()\n",
    "\n",
    "env_dx = np.array([pong_env.DISPLAY_WIDTH / numStates[0], pong_env.DISPLAY_HEIGHT / numStates[1]])\n",
    "\n",
    "# Train a policy. TODO: save it\n",
    "sol_policy = train_q_learning(env)\n",
    "\n",
    "with open(save_path, 'wb') as f:\n",
    "    np.save(f, sol_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play  simulation with the learned policy\n",
    "reward, images = run_episode(env, sol_policy, True)\n",
    "print(reward)\n",
    "\n",
    "frame = images[0]\n",
    "height, width, layers = frame.shape\n",
    "\n",
    "video = cv2.VideoWriter('demo.avi', 0, 30, (width,height))\n",
    "\n",
    "for image in images:\n",
    "    video.write(image)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
